{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            style                     sentence\n",
      "0          formal       ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”.\n",
      "1          formal     ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”?\n",
      "2          formal  ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”.\n",
      "3          formal       ê°€ì¥ ë‚˜ì´ê°€ ë§ì€ ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”?\n",
      "4          formal           ì—¬ì„¯ ì‚´ì…ë‹ˆë‹¤. ê°ˆìƒ‰ ê³ ì–‘ì´ì—ìš”.\n",
      "...           ...                          ...\n",
      "62980  translator                          NaN\n",
      "62981  translator                          NaN\n",
      "62982  translator                          NaN\n",
      "62983  translator                          NaN\n",
      "62984  translator                          NaN\n",
      "\n",
      "[62985 rows x 2 columns]\n",
      "            style                                 sentence  label\n",
      "0          formal                   ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”.      6\n",
      "1          formal                 ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”?      6\n",
      "2          formal              ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”.      6\n",
      "3          formal                   ê°€ì¥ ë‚˜ì´ê°€ ë§ì€ ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”?      6\n",
      "4          formal                       ì—¬ì„¯ ì‚´ì…ë‹ˆë‹¤. ê°ˆìƒ‰ ê³ ì–‘ì´ì—ìš”.      6\n",
      "...           ...                                      ...    ...\n",
      "36789  translator                        ì§„í–‰ë˜ëŠ” ê³µë¶€ì˜ ì¢…ë¥˜ëŠ” ë¬´ì—‡ì¸?     16\n",
      "36790  translator              ë‚˜ëŠ” ìˆ˜í•™, ì˜ì–´, êµ­ì–´ì™€ ê°™ì€ ìˆ˜ì—…ë“¤ì„ ë“£ëŠ”ë‹¤.     16\n",
      "36791  translator                      ë‹¹ì‹ ì´ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ìˆ˜ì—…ì€ ë¬´ì—‡?     16\n",
      "36792  translator                ê°œì¸ì ìœ¼ë¡œ, ë‚˜ëŠ” ì²´ìœ¡ ìˆ˜ì—…ì˜ í° ì• í˜¸ê°€ì´ë‹¤.     16\n",
      "36793  translator  ê·¸ê²ƒì€ ì¬ë¯¸ìˆëŠ” ê²ƒìœ¼ë¡œ ë“¤ë¦¬ëŠ”, ë‚˜ ë˜í•œ í•™êµì—ì„œ ì¶•êµ¬ë¥¼ ìì£¼ ì¦ê²¼ë‹¤.     16\n",
      "\n",
      "[36794 rows x 3 columns]\n",
      "ë¼ë²¨ ê°œìˆ˜: 17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"smilestyle_dataset.tsv\", sep=\"\\t\")\n",
    "\n",
    "# wide -> long í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì—´ ì´ë¦„ = ìŠ¤íƒ€ì¼ â†’ í•˜ë‚˜ì˜ ì—´ë¡œ ë³€í™˜)\n",
    "df_long = df.melt(var_name=\"style\", value_name=\"sentence\")\n",
    "print(df_long)\n",
    "\n",
    "# ë¼ë²¨ ì¸ì½”ë”©\n",
    "le = LabelEncoder()\n",
    "df_long['label'] = le.fit_transform(df_long['style'])  # ìŠ¤íƒ€ì¼ì„ ìˆ«ìë¡œ ë³€í™˜\n",
    "df_long = df_long.dropna(subset=[\"sentence\"]).reset_index(drop=True)\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "\n",
    "# í™•ì¸\n",
    "print(df_long)\n",
    "print(\"ë¼ë²¨ ê°œìˆ˜:\", num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29435/29435 [00:03<00:00, 7855.85 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7359/7359 [00:00<00:00, 7590.17 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'style': 'king', 'sentence': 'ì°¸ìœ¼ë¡œ ì‚¬ì•…í•œ ì¥ì‚¬ì¸ì§€ê³ !', 'labels': tensor(12), '__index_level_0__': tensor(28515), 'input_ids': tensor([    2, 10133, 13783,  9774,  8148,  4034,     5,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì¦ˆ í•¨ìˆ˜ ìˆ˜ì • (sentence ì—´ ì‚¬ìš©)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# 8:2ë¡œ ë‚˜ëˆ„ê¸° (ë¼ë²¨ ë¹„ìœ¨ ìœ ì§€, ì¬í˜„ì„± ìˆëŠ” ë¶„í• )\n",
    "train_df, val_df = train_test_split(\n",
    "    df_long,\n",
    "    test_size=0.2,\n",
    "    stratify=df_long[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Hugging Face Dataset ê°ì²´ë¡œ ë³€í™˜\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§•\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_val = tokenized_val.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_val.set_format(\"torch\")\n",
    "\n",
    "print(tokenized_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"beomi/kcbert-base\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 307,985 || all params: 109,239,586 || trainable%: 0.2819\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS  # ë¬¸ì¥ ë¶„ë¥˜\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ê¹€ì†Œì—°\\ToneDetect\\venv\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"ToneDetect_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
