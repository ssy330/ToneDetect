{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "befa9c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ìœ ì •ì•™ ì•„ì§ ì¶œë°œ ì•ˆí–ˆìœ¼ë©´ í•œ 20ë¶„ë§Œ ë¯¸ë£°ìˆ˜ ì´ì“°ê¹Œ..ã… ã… ', 'í•˜ë˜ê²Œ ì•ˆëë‚¬ì–´.. ì˜ë¦¬..', 'í—‰ ì•„ëƒì•„ëƒ!!?', 'ì—¬ìœ ìˆê²Œ ì™€ ã…ã…', '30ë¶„ì¯¤ ë³´ìŸ!!', 'ì•— ê³ ë§ˆì–´.. ê·¸ë•Œ ë³´ìŸ!!', 'ë‚˜ ë„ì°©í–ˆì–´!!', 'ì–´ë””ì„œ ë³¼ê¹Œ??', 'í—‰ !!! ë‚˜ëŠ” ë²„ìŠ¤ê°€ ëŠ¦ì–´ì ¸ì„œ ê³§ íƒˆ ê±° ê°™ì•„...ã…ã…', 'ê·¸ ìŠ¤íƒ€ë²…ìŠ¤ ê°ˆê¹Œ í•˜ëŠ”ë° ì–´ë•Œ??', '[ë„¤ì´ë²„ ì§€ë„]', 'êµ¬ë­ ê·¸ìª½ìœ¼ë¡œ ê°€ê³ ìˆì„ê²Œ!!', 'ì›…ì›… ê³ ë§ˆì–´ğŸ¥°ğŸ¥°', 'ì–¸ëŠ¥ ê°ˆê²Œ!!!', 'ë‚˜ ë‚´ë ¸ì˜¤ ã…ã…', 'ìŠ¤ë²… ì•ì´ì–‘??', 'ì›…ì›…', 'ê·¼ë° ì—¬ê¸° ìë¦¬ê°€ ì—…ë”°..', 'ë¯¸ì¹œ', 'ê±´ë„ˆí¸ì— íˆ¬ì¸ ìˆëŠ”ë° ê°€ë³¼ê¹Œ???', 'ì•„ ã…‡ã…‹ ë³´ì¸ë‹¤', 'ë‚´ê°€ ì§€ê¸ˆ íˆ¬ì¸ ê·¼ì²œë°', 'ì•„í•˜', 'ê±´ë„ˆ ì˜¬ë­?????', 'ì›…ì›…', 'ì•Œì¨ğŸ˜†ğŸ˜†', 'íŒŒì¼: KakaoTalk_20250511_1625_29_658_ìœ ì •ìœ ì •.txt', 'TF/TF-IDF ê¸°ë°˜', 'ì‚¬ì§„', '3,500ì›ì„ ë³´ëƒˆì–´ìš”.', '3,500ì›ì„ ë°›ì•˜ì–´ìš”.', 'https://www.genspark.ai/', 'gimsoyeon092@gmail.com', 'ì–¸ë‹ˆ ì§„ì§œ ë„ˆë¬´ ê³ ìƒ¹í•´ì¨... ì¡°ì‹¬íˆ ì˜ ë“¤ì–´ê°€êµ¬ í™”ìšœì— ë³´ìğŸ˜‚ğŸ˜‚â¤', 'ë„ˆë‘ ì˜¤ëŠ˜ ë„ˆë¬´ë„ˆë¬´ ê³ ìƒí•´ì¨..!! í‘¹ ì‰¬ê³  í™”ìš”ì¼ì— ë³´ìŸ~', 'ì–¸ë‹ˆ ê·¼ë° ìš°ë¦¬ ì•„ì§ ìì£¼ ì“°ëŠ” ë‹¨ì–´ë‘ ì¢…ê²°ì–´ë¯¸ ë¶„ì„í•˜ëŠ” ê±´ êµ¬í˜„ì´ ì•ˆ ë˜ì–´ ìˆìë‚˜ ì´ê±´ ì¶”í›„ì— í´ëŸ¬ìŠ¤í„°ë§ ì´ìš©í•´ì„œ ì¶”ê°€í• ê±°ë¼ê³  ë§í•˜ëŠ” ê²Œ ì¢‹ì„ ê±° ê°™ì€ë° ì˜¤ë•Œ??', 'ì›… ì¢‹ì•„ì¢‹ì•„~ ì¶”í›„ ì§„í–‰í•  ë¶€ë¶„ì—ì„œ LDA ë§í• ë•Œ í•˜ë©´ ë˜ê² ë‹¤!', 'ì˜¤ì¼€ì´!! ê³ ë§ˆì›¡ğŸ¥°', 'ì–¸ë‹ˆ !! ë‚˜ ëŒ€ë³¸ ë‹¤ì¼ì–´ ã…ã…', 'ì½ì–´ë´¤ì–´!! ê³ ìƒí•´ì¨~ ì´ëŒ€ë¡œ í•˜ë©´ ë ê²ƒ ê°™ì€ë°!?', 'ì˜¤ ì§„ì§œ??? ì¡°ì•„ì¡°ì•„ ê³ ë§ˆì–´ ã…ã…ã…ã…', 'ë‚´ì¼ ì´ëŒ€ë¡œ í• ê²¡ ã…ã…', 'ì–¸ë‹ˆ í˜¹ì‹œ ì½”ë“œ ë¶„ì„ ì–¼ë§ˆë‚˜ í–ˆì–´?? ë‚´ê°€ ì•„ì§ í•™êµë¼ì„œ ì§‘ ê°€ì„œ í•  ìˆ˜ ìˆì„ ê±° ê°™ì•„â€¦.', 'ê³§ ì§‘ìœ¼ë¡œ ì¶œë°œí•´..ã… ã… ', 'ì•„ í— ë‚˜ ì˜¤ëŠ˜ ê³„ì† ìë²„ë ¸ë„¤.. ì§€ê¸ˆë¶€í„° í•˜ê³  ìˆì„ê²Œã… ã… ', 'ìœ ì •ì•™ ì˜¤ëŠ˜êº¼ëŠ” ë‚´ê°€ í•´ì„œ ì œì¶œí• ê²¡~ ê¸ˆë°© í• ìˆ˜ ìˆì„ê²ƒ ê°™ì•„!! ëŒ€ë³¸ ì“°ëŠë¼ ê³ ìƒí–ˆìœ¼ë‹ˆê¹Œ ì‰¬ì–´!', 'ì—...? ì§„ì§œ ê´œì°®ê²Œì¨?? ë‚´ ì˜¤ì›€ì´ í•„ìš”ã…˜ë©´ ê¼­ ë§í•´ì¤˜!!!!!', 'ë„ìŒ', 'ìœ ì •ì•„ ë‚˜ 2ë²ˆ ì•„ë¬´ë¦¬ í•´ë„ ì‹¤í–‰ì´ ì•ˆë˜ëŠ”ë° í˜¹ì‹œ ì‹¤í–‰ë˜ë©´ ìº¡ì³ë§Œ ì¢€ ë³´ë‚´ì¤„ìˆ˜ ìˆì„ê¹Œ?ã… ã… ', 'ë‚˜ë„ ê³„ì† í•´ë³´ê³  ìˆì–´..!!', 'ì–´ ë­ì•¼', 'ê°‘ìê¸° ëë‹¤..ã…‹ã…‹ã…‹ã…‹ã…‹', 'ì œì¶œí• ê²¡!!', 'ì–´ë¨¸ ì–¸ë‹ˆ.... ì§„ì§œ ê³ ë§ˆì›Œ....', 'ì–´ì œ ê°€ìë§ˆì ìë²„ë ¸ì–´... í†¡ ëª» ë´ì„œ ë¯¸ì•ˆí•´ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­ğŸ˜­', 'ì´ë”°ë³´ìŸ,, ì§„ì§œ ê³ ë§™êµ¬ ìˆ˜ê³ í•´ì¨ğŸ˜‚â¤â¤', 'ë°œí‘œ ìë£ŒëŠ” ë‚´ê°€ ë°©ê¸ˆ ë‚´ì½', 'ì›…ì›… ê³ ìƒí•´ì¨~!!', 'https://sfida.tistory.com/46']\n",
      "[['ìœ ì •', 'ì¶œë°œ', 'ë¶„ë§Œ'], [], [], ['ì—¬ìœ '], ['ë³´ìŸ'], ['ê³ ë§ˆ', 'ê·¸ë•Œ', 'ë³´ìŸ'], ['ë„ì°©'], [], ['ë²„ìŠ¤'], ['ìŠ¤íƒ€ë²…ìŠ¤'], ['ë„¤ì´ë²„', 'ì§€ë„'], ['êµ¬ë­'], ['ì›…ì›…', 'ê³ ë§ˆ'], [], [], ['ìŠ¤ë²…', 'ì´ì–‘'], ['ì›…ì›…'], ['ì—¬ê¸°', 'ìë¦¬'], [], ['ê±´ë„ˆí¸', 'íˆ¬ì¸'], [], ['ì§€ê¸ˆ', 'íˆ¬ì¸'], ['ì•„í•˜'], ['ê±´ë„ˆ', 'ì˜¬ë­'], ['ì›…ì›…'], [], ['íŒŒì¼', 'ìœ ì •', 'ìœ ì •'], ['ê¸°ë°˜'], ['ì‚¬ì§„'], [], [], [], [], ['ì–¸ë‹ˆ', 'ì§„ì§œ', 'ì¡°ì‹¬', 'í™”ìšœ'], ['ì˜¤ëŠ˜', 'ê³ ìƒ', 'í™”ìš”ì¼', 'ë³´ìŸ'], ['ì–¸ë‹ˆ', 'ìš°ë¦¬', 'ìì£¼', 'ë‹¨ì–´', 'ì¢…ê²°ì–´ë¯¸', 'ë¶„ì„', 'êµ¬í˜„', 'ì´ê±´', 'ì¶”í›„', 'í´ëŸ¬ìŠ¤í„°ë§', 'ì´ìš©', 'ì¶”ê°€'], ['ì¶”í›„', 'ì§„í–‰', 'ë¶€ë¶„'], ['ì˜¤ì¼€ì´', 'ê³ ë§ˆ'], ['ì–¸ë‹ˆ', 'ëŒ€ë³¸'], ['ê³ ìƒ'], ['ì§„ì§œ', 'ê³ ë§ˆ'], ['ë‚´ì¼', 'í• ê²¡'], ['ì–¸ë‹ˆ', 'í˜¹ì‹œ', 'ì½”ë“œ', 'ë¶„ì„', 'ì–¼ë§ˆë‚˜', 'í•™êµ'], ['ì¶œë°œ'], ['ì˜¤ëŠ˜', 'ê³„ì†', 'ì§€ê¸ˆ'], ['ìœ ì •', 'ì˜¤ëŠ˜', 'ì œì¶œ', 'í• ê²¡', 'ê¸ˆë°©', 'ëŒ€ë³¸', 'ê³ ìƒ'], ['ì§„ì§œ', 'í•„ìš”'], ['ë„ìŒ'], ['ìœ ì •ì•„', 'í•´ë„', 'ì‹¤í–‰', 'í˜¹ì‹œ', 'ì‹¤í–‰', 'ìº¡ì³'], ['ê³„ì†'], [], ['ê°‘ìê¸°'], ['ì œì¶œ', 'í• ê²¡'], ['ì–´ë¨¸', 'ì–¸ë‹ˆ', 'ì§„ì§œ'], ['ì–´ì œ'], ['ì´ë”°', 'ë³´ìŸ', 'ì§„ì§œ', 'ìˆ˜ê³ '], ['ë°œí‘œ', 'ìë£Œ', 'ë°©ê¸ˆ', 'ë‚´ì½'], ['ì›…ì›…', 'ê³ ìƒ'], []]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from gensim import corpora\n",
    "import re\n",
    "\n",
    "# 1. íŒŒì¼ ì½ê¸°\n",
    "with open(\"KakaoTalk_20250515_0053_22_930_ìœ ì •ìœ ì •.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# 2. ë©”ì‹œì§€ ì¶”ì¶œ (ë‚ ì§œ/ì´ë¦„/ì‹œê°„ ì œê±°)\n",
    "# í˜•ì‹ ì˜ˆì‹œ: [ê¹€ì†Œì—°] [ì˜¤í›„ 3:36] ì •ì‚°í•˜ê¸°ë¥¼ ìš”ì²­í–ˆì–´ìš”.\n",
    "messages = re.findall(r'\\[.+?\\] \\[.+?\\] (.+)', raw_text)\n",
    "\n",
    "print(messages)\n",
    "\n",
    "# 3. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜  \n",
    "okt = Okt()\n",
    "\n",
    "def preprocess(text):\n",
    "    # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    text = re.sub(r'[^ê°€-í£\\s]', '', text)\n",
    "    # ëª…ì‚¬ë§Œ ì¶”ì¶œ\n",
    "    nouns = okt.nouns(text)\n",
    "    # ê¸¸ì´ 1 ì´í•˜ ì œê±°\n",
    "    return [word for word in nouns if len(word) > 1]\n",
    "\n",
    "# 4. ì „ì²´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "tokenized_docs = [preprocess(msg) for msg in messages if msg.strip()]\n",
    "\n",
    "# 5. ë”•ì…”ë„ˆë¦¬ ë° ì½”í¼ìŠ¤ ìƒì„±\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# ì „ì²˜ë¦¬ ê²°ê³¼ í™•ì¸\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46d707db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklue/bert-base\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# âœ… ë°˜ë“œì‹œ NER í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©\u001b[39;00m\n\u001b[0;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 4. íŒŒì´í”„ë¼ì¸ ìƒì„±\u001b[39;00m\n\u001b[0;32m     17\u001b[0m ner_pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, grouped_entities\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\yhshi\\ToneDetect\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1840\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1840\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yhshi\\ToneDetect\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1828\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1826\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# 1. íŒŒì¼ ì½ê¸°\n",
    "with open(\"KakaoTalk_20250515_0053_22_930_ìœ ì •ìœ ì •.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# 2. ë©”ì‹œì§€ ì¶”ì¶œ\n",
    "messages = re.findall(r'\\[.+?\\] \\[.+?\\] (.+)', raw_text)\n",
    "\n",
    "# 3. NER ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë”©\n",
    "model_name = \"klue/bert-base\"  # âœ… ë°˜ë“œì‹œ NER í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# 4. íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
    "ner_pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "\n",
    "# 5. ë©”ì‹œì§€ë³„ ëª…ì‚¬ ì¶”ì¶œ\n",
    "for message in messages:\n",
    "    ner_results = ner_pipe(message)\n",
    "    for entity in ner_results:\n",
    "        word = entity['word']\n",
    "        label = entity['entity_group']\n",
    "        if label in ['PER', 'ORG', 'LOC']:  # ì¸ë¬¼, ì¡°ì§, ì¥ì†Œ\n",
    "            print(f\"ëª…ì‚¬ í›„ë³´: {word} (ë¼ë²¨: {label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90e7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© ì£¼ì œ 1: 0.073*\"ë³´ìŸ\" + 0.050*\"ê³ ë§ˆ\" + 0.050*\"ì§„ì§œ\" + 0.027*\"ì–¸ë‹ˆ\" + 0.027*\"ë¶„ì„\"\n",
      "ğŸ§© ì£¼ì œ 2: 0.032*\"ê³ ìƒ\" + 0.032*\"í• ê²¡\" + 0.032*\"ì–¸ë‹ˆ\" + 0.032*\"ëŒ€ë³¸\" + 0.032*\"ì œì¶œ\"\n",
      "ğŸ§© ì£¼ì œ 3: 0.053*\"ê³ ë§ˆ\" + 0.053*\"í• ê²¡\" + 0.053*\"ë¶€ë¶„\" + 0.053*\"ì§„í–‰\" + 0.053*\"ì˜¤ì¼€ì´\"\n",
      "ğŸ§© ì£¼ì œ 4: 0.061*\"ì§„ì§œ\" + 0.061*\"ìœ ì •\" + 0.061*\"ê³ ìƒ\" + 0.061*\"ì›…ì›…\" + 0.042*\"ì‹¤í–‰\"\n",
      "ğŸ§© ì£¼ì œ 5: 0.045*\"íˆ¬ì¸\" + 0.045*\"ì›…ì›…\" + 0.045*\"ì§€ê¸ˆ\" + 0.045*\"ì¶œë°œ\" + 0.045*\"ë‚´ì¼\"\n",
      "ğŸ“ LDA ê²°ê³¼ê°€ lda_kakaotalk_result.html íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 6. LDA ëª¨ë¸ í•™ìŠµ\n",
    "lda_model = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=5,         # ì¶”ì¶œí•  ì£¼ì œ ìˆ˜\n",
    "    random_state=42,\n",
    "    passes=10,            # ë°˜ë³µ íšŸìˆ˜\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "# 7. ì£¼ì œ ì¶œë ¥\n",
    "for idx, topic in lda_model.print_topics(num_words=5):\n",
    "    print(f\"ğŸ§© ì£¼ì œ {idx + 1}: {topic}\")\n",
    "\n",
    "# 8. pyLDAvisë¡œ ì‹œê°í™”\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.save_html(vis, 'lda_kakaotalk_result.html')\n",
    "print(\"ğŸ“ LDA ê²°ê³¼ê°€ lda_kakaotalk_result.html íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
