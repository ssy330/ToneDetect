{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "befa9c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['유정앙 아직 출발 안했으면 한 20분만 미룰수 이쓰까..ㅠㅠ', '하던게 안끝났어.. 쏘리..', '헉 아냐아냐!!?', '여유있게 와 ㅎㅎ', '30분쯤 보쟝!!', '앗 고마어.. 그때 보쟝!!', '나 도착했어!!', '어디서 볼까??', '헉 !!! 나는 버스가 늦어져서 곧 탈 거 같아...ㅎㅎ', '그 스타벅스 갈까 하는데 어때??', '[네이버 지도]', '구랭 그쪽으로 가고있을게!!', '웅웅 고마어🥰🥰', '언능 갈게!!!', '나 내렸오 ㅎㅎ', '스벅 앞이양??', '웅웅', '근데 여기 자리가 업따..', '미친', '건너편에 투썸 있는데 가볼까???', '아 ㅇㅋ 보인다', '내가 지금 투썸 근천데', '아하', '건너 올랭?????', '웅웅', '알써😆😆', '파일: KakaoTalk_20250511_1625_29_658_유정유정.txt', 'TF/TF-IDF 기반', '사진', '3,500원을 보냈어요.', '3,500원을 받았어요.', 'https://www.genspark.ai/', 'gimsoyeon092@gmail.com', '언니 진짜 너무 고샹해써... 조심히 잘 들어가구 화욜에 보자😂😂❤', '너두 오늘 너무너무 고생해써..!! 푹 쉬고 화요일에 보쟝~', '언니 근데 우리 아직 자주 쓰는 단어랑 종결어미 분석하는 건 구현이 안 되어 있자나 이건 추후에 클러스터링 이용해서 추가할거라고 말하는 게 좋을 거 같은데 오때??', '웅 좋아좋아~ 추후 진행할 부분에서 LDA 말할때 하면 되겠다!', '오케이!! 고마웡🥰', '언니 !! 나 대본 다썼어 ㅎㅎ', '읽어봤어!! 고생해써~ 이대로 하면 될것 같은데!?', '오 진짜??? 조아조아 고마어 ㅎㅎㅎㅎ', '내일 이대로 할겡 ㅎㅎ', '언니 혹시 코드 분석 얼마나 했어?? 내가 아직 학교라서 집 가서 할 수 있을 거 같아….', '곧 집으로 출발해..ㅠㅠ', '아 헐 나 오늘 계속 자버렸네.. 지금부터 하고 있을게ㅠㅠ', '유정앙 오늘꺼는 내가 해서 제출할겡~ 금방 할수 있을것 같아!! 대본 쓰느라 고생했으니까 쉬어!', '에...? 진짜 괜찮게써?? 내 오움이 필요ㅘ면 꼭 말해줘!!!!!', '도음', '유정아 나 2번 아무리 해도 실행이 안되는데 혹시 실행되면 캡쳐만 좀 보내줄수 있을까?ㅠㅠ', '나도 계속 해보고 있어..!!', '어 뭐야', '갑자기 됐다..ㅋㅋㅋㅋㅋ', '제출할겡!!', '어머 언니.... 진짜 고마워....', '어제 가자마자 자버렸어... 톡 못 봐서 미안해😭😭😭😭😭', '이따보쟝,, 진짜 고맙구 수고해써😂❤❤', '발표 자료는 내가 방금 내썽', '웅웅 고생해써~!!', 'https://sfida.tistory.com/46']\n",
      "[['유정', '출발', '분만'], [], [], ['여유'], ['보쟝'], ['고마', '그때', '보쟝'], ['도착'], [], ['버스'], ['스타벅스'], ['네이버', '지도'], ['구랭'], ['웅웅', '고마'], [], [], ['스벅', '이양'], ['웅웅'], ['여기', '자리'], [], ['건너편', '투썸'], [], ['지금', '투썸'], ['아하'], ['건너', '올랭'], ['웅웅'], [], ['파일', '유정', '유정'], ['기반'], ['사진'], [], [], [], [], ['언니', '진짜', '조심', '화욜'], ['오늘', '고생', '화요일', '보쟝'], ['언니', '우리', '자주', '단어', '종결어미', '분석', '구현', '이건', '추후', '클러스터링', '이용', '추가'], ['추후', '진행', '부분'], ['오케이', '고마'], ['언니', '대본'], ['고생'], ['진짜', '고마'], ['내일', '할겡'], ['언니', '혹시', '코드', '분석', '얼마나', '학교'], ['출발'], ['오늘', '계속', '지금'], ['유정', '오늘', '제출', '할겡', '금방', '대본', '고생'], ['진짜', '필요'], ['도음'], ['유정아', '해도', '실행', '혹시', '실행', '캡쳐'], ['계속'], [], ['갑자기'], ['제출', '할겡'], ['어머', '언니', '진짜'], ['어제'], ['이따', '보쟝', '진짜', '수고'], ['발표', '자료', '방금', '내썽'], ['웅웅', '고생'], []]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from gensim import corpora\n",
    "import re\n",
    "\n",
    "# 1. 파일 읽기\n",
    "with open(\"KakaoTalk_20250515_0053_22_930_유정유정.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# 2. 메시지 추출 (날짜/이름/시간 제거)\n",
    "# 형식 예시: [김소연] [오후 3:36] 정산하기를 요청했어요.\n",
    "messages = re.findall(r'\\[.+?\\] \\[.+?\\] (.+)', raw_text)\n",
    "\n",
    "print(messages)\n",
    "\n",
    "# 3. 전처리 함수 정의  \n",
    "okt = Okt()\n",
    "\n",
    "def preprocess(text):\n",
    "    # 특수문자 제거\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)\n",
    "    # 명사만 추출\n",
    "    nouns = okt.nouns(text)\n",
    "    # 길이 1 이하 제거\n",
    "    return [word for word in nouns if len(word) > 1]\n",
    "\n",
    "# 4. 전체 문서 리스트 생성\n",
    "tokenized_docs = [preprocess(msg) for msg in messages if msg.strip()]\n",
    "\n",
    "# 5. 딕셔너리 및 코퍼스 생성\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# 전처리 결과 확인\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46d707db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklue/bert-base\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# ✅ 반드시 NER 학습된 모델 사용\u001b[39;00m\n\u001b[0;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 4. 파이프라인 생성\u001b[39;00m\n\u001b[0;32m     17\u001b[0m ner_pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, grouped_entities\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\yhshi\\ToneDetect\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1840\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1840\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yhshi\\ToneDetect\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1828\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1826\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# 1. 파일 읽기\n",
    "with open(\"KakaoTalk_20250515_0053_22_930_유정유정.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# 2. 메시지 추출\n",
    "messages = re.findall(r'\\[.+?\\] \\[.+?\\] (.+)', raw_text)\n",
    "\n",
    "# 3. NER 사전학습 모델 로딩\n",
    "model_name = \"klue/bert-base\"  # ✅ 반드시 NER 학습된 모델 사용\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# 4. 파이프라인 생성\n",
    "ner_pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "\n",
    "# 5. 메시지별 명사 추출\n",
    "for message in messages:\n",
    "    ner_results = ner_pipe(message)\n",
    "    for entity in ner_results:\n",
    "        word = entity['word']\n",
    "        label = entity['entity_group']\n",
    "        if label in ['PER', 'ORG', 'LOC']:  # 인물, 조직, 장소\n",
    "            print(f\"명사 후보: {word} (라벨: {label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90e7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 주제 1: 0.073*\"보쟝\" + 0.050*\"고마\" + 0.050*\"진짜\" + 0.027*\"언니\" + 0.027*\"분석\"\n",
      "🧩 주제 2: 0.032*\"고생\" + 0.032*\"할겡\" + 0.032*\"언니\" + 0.032*\"대본\" + 0.032*\"제출\"\n",
      "🧩 주제 3: 0.053*\"고마\" + 0.053*\"할겡\" + 0.053*\"부분\" + 0.053*\"진행\" + 0.053*\"오케이\"\n",
      "🧩 주제 4: 0.061*\"진짜\" + 0.061*\"유정\" + 0.061*\"고생\" + 0.061*\"웅웅\" + 0.042*\"실행\"\n",
      "🧩 주제 5: 0.045*\"투썸\" + 0.045*\"웅웅\" + 0.045*\"지금\" + 0.045*\"출발\" + 0.045*\"내일\"\n",
      "📁 LDA 결과가 lda_kakaotalk_result.html 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 6. LDA 모델 학습\n",
    "lda_model = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=5,         # 추출할 주제 수\n",
    "    random_state=42,\n",
    "    passes=10,            # 반복 횟수\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "# 7. 주제 출력\n",
    "for idx, topic in lda_model.print_topics(num_words=5):\n",
    "    print(f\"🧩 주제 {idx + 1}: {topic}\")\n",
    "\n",
    "# 8. pyLDAvis로 시각화\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.save_html(vis, 'lda_kakaotalk_result.html')\n",
    "print(\"📁 LDA 결과가 lda_kakaotalk_result.html 파일로 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
