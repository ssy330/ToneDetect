{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            style                     sentence\n",
      "0          formal       ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”.\n",
      "1          formal     ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”?\n",
      "2          formal  ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”.\n",
      "3          formal       ê°€ì¥ ë‚˜ì´ê°€ ë§ì€ ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”?\n",
      "4          formal           ì—¬ì„¯ ì‚´ì…ë‹ˆë‹¤. ê°ˆìƒ‰ ê³ ì–‘ì´ì—ìš”.\n",
      "...           ...                          ...\n",
      "62980  translator                          NaN\n",
      "62981  translator                          NaN\n",
      "62982  translator                          NaN\n",
      "62983  translator                          NaN\n",
      "62984  translator                          NaN\n",
      "\n",
      "[62985 rows x 2 columns]\n",
      "            style                                 sentence  label\n",
      "0          formal                   ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”.      6\n",
      "1          formal                 ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”?      6\n",
      "2          formal              ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”.      6\n",
      "3          formal                   ê°€ì¥ ë‚˜ì´ê°€ ë§ì€ ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”?      6\n",
      "4          formal                       ì—¬ì„¯ ì‚´ì…ë‹ˆë‹¤. ê°ˆìƒ‰ ê³ ì–‘ì´ì—ìš”.      6\n",
      "...           ...                                      ...    ...\n",
      "36789  translator                        ì§„í–‰ë˜ëŠ” ê³µë¶€ì˜ ì¢…ë¥˜ëŠ” ë¬´ì—‡ì¸?     16\n",
      "36790  translator              ë‚˜ëŠ” ìˆ˜í•™, ì˜ì–´, êµ­ì–´ì™€ ê°™ì€ ìˆ˜ì—…ë“¤ì„ ë“£ëŠ”ë‹¤.     16\n",
      "36791  translator                      ë‹¹ì‹ ì´ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ìˆ˜ì—…ì€ ë¬´ì—‡?     16\n",
      "36792  translator                ê°œì¸ì ìœ¼ë¡œ, ë‚˜ëŠ” ì²´ìœ¡ ìˆ˜ì—…ì˜ í° ì• í˜¸ê°€ì´ë‹¤.     16\n",
      "36793  translator  ê·¸ê²ƒì€ ì¬ë¯¸ìˆëŠ” ê²ƒìœ¼ë¡œ ë“¤ë¦¬ëŠ”, ë‚˜ ë˜í•œ í•™êµì—ì„œ ì¶•êµ¬ë¥¼ ìì£¼ ì¦ê²¼ë‹¤.     16\n",
      "\n",
      "[36794 rows x 3 columns]\n",
      "ë¼ë²¨ ê°œìˆ˜: 17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"smilestyle_dataset.tsv\", sep=\"\\t\")\n",
    "\n",
    "# ë³‘í•© ê¸°ì¤€ ì •ì˜\n",
    "merge_map = {\n",
    "    \"formal\": \"formal\",\n",
    "    \"seonbi\": \"formal\",\n",
    "    \"translator\": \"formal\",\n",
    "\n",
    "    \"informal\": \"informal\",\n",
    "    \"azae\": \"informal\",\n",
    "    \"choding\": \"informal\",\n",
    "    \"joongding\": \"informal\",\n",
    "\n",
    "    \"chat\": \"chat_emoticon\",\n",
    "    \"emoticon\": \"chat_emoticon\",\n",
    "    \"enfp\": \"chat_emoticon\",\n",
    "\n",
    "    \"gentle\": \"soft_polite\",\n",
    "    \"sosim\": \"soft_polite\",\n",
    "\n",
    "    \"halbae\": \"elder_speech\",\n",
    "    \"halmae\": \"elder_speech\",\n",
    "}\n",
    "\n",
    "# ì‚­ì œ ëŒ€ìƒ ë¼ë²¨\n",
    "drop_labels = [\"king\", \"naruto\", \"android\"]\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë³‘í•© ë° ì‚­ì œ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def flatten_and_merge_labels(df):\n",
    "    records = []\n",
    "    for col in df.columns:\n",
    "        if col in drop_labels:\n",
    "            continue\n",
    "        merged_label = merge_map.get(col, None)\n",
    "        if merged_label:\n",
    "            for sentence in df[col].dropna():\n",
    "                records.append({\"label\": merged_label, \"sentence\": sentence})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# ì ìš©\n",
    "df = pd.read_csv(\"smilestyle_dataset.tsv\", sep=\"\\t\")\n",
    "df = flatten_and_merge_labels(df)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # ë°ì´í„° ì„ê¸°\n",
    "\n",
    "# ë¼ë²¨ ì¸ì½”ë”©\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])  # ìŠ¤íƒ€ì¼ì„ ìˆ«ìë¡œ ë³€í™˜\n",
    "df = df.dropna(subset=[\"sentence\"]).reset_index(drop=True)\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "\n",
    "# í™•ì¸\n",
    "print(df.head())\n",
    "print(\"ë¼ë²¨ ê°œìˆ˜:\", num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29435/29435 [00:03<00:00, 7855.85 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7359/7359 [00:00<00:00, 7590.17 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'style': 'king', 'sentence': 'ì°¸ìœ¼ë¡œ ì‚¬ì•…í•œ ì¥ì‚¬ì¸ì§€ê³ !', 'labels': tensor(12), '__index_level_0__': tensor(28515), 'input_ids': tensor([    2, 10133, 13783,  9774,  8148,  4034,     5,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì¦ˆ í•¨ìˆ˜ ìˆ˜ì • (sentence ì—´ ì‚¬ìš©)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# 8:2ë¡œ ë‚˜ëˆ„ê¸° (ë¼ë²¨ ë¹„ìœ¨ ìœ ì§€, ì¬í˜„ì„± ìˆëŠ” ë¶„í• )\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Hugging Face Dataset ê°ì²´ë¡œ ë³€í™˜\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§•\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_val = tokenized_val.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_val.set_format(\"torch\")\n",
    "\n",
    "example = tokenized_train.with_format(\"python\")[0]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"beomi/kcbert-base\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 307,985 || all params: 109,239,586 || trainable%: 0.2819\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS  # ë¬¸ì¥ ë¶„ë¥˜\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ê¹€ì†Œì—°\\ToneDetect\\venv\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â‘  Base model ì €ì¥ (ë¡œë¼ adapter ì ìš© ì „ ì›ë˜ ëª¨ë¸)\n",
    "base_model = model.base_model.model\n",
    "base_model.save_pretrained(\"ToneDetect_base\")\n",
    "\n",
    "# â‘¡ Adapter ê°€ì¤‘ì¹˜ + tokenizer ì €ì¥\n",
    "model.save_pretrained(\"ToneDetect_adapter\")\n",
    "tokenizer.save_pretrained(\"ToneDetect_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ë‹¤ìŒì€ ì •í™•ë„ ê²€ì¦ìš© ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# PEFT ì ìš©ëœ ëª¨ë¸ ë¡œë“œ (ì´ì „ í•™ìŠµëœ ê²ƒ)\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "adapter_path = \"/content/drive/MyDrive/Models/ToneDetect_adapter\"\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"beomi/kcbert-base\", num_labels=5)\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "model.eval()\n",
    "\n",
    "# validation ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"smilestyle_dataset.tsv\", sep=\"\\t\")\n",
    "# merge ë° ì „ì²˜ë¦¬ ì‘ì—… ìƒëµ â€“ ê¸°ì¡´ ì½”ë“œë¡œ flatten_and_merge_labels í•¨ìˆ˜ ì‚¬ìš©\n",
    "\n",
    "# ì „ì²˜ë¦¬\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = flatten_and_merge_labels(df)\n",
    "df = df.dropna(subset=[\"sentence\"]).reset_index(drop=True)\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"label\"])\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "# ì—­ë³€í™˜ì„ ìœ„í•œ ë§µ\n",
    "idx2label = dict(zip(range(len(le.classes_)), le.inverse_transform(range(len(le.classes_)))))\n",
    "\n",
    "# ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "predictions = []\n",
    "for i, row in tqdm(val_df.iterrows(), total=len(val_df)):\n",
    "    inputs = tokenizer(row[\"sentence\"], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    predictions.append(pred)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ ë° ì •í™•ë„ ê³„ì‚°\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "val_df[\"gold_label\"] = val_df[\"label\"].map(idx2label)\n",
    "val_df[\"pred_label\"] = [idx2label[p] for p in predictions]\n",
    "val_df[\"correct\"] = val_df[\"gold_label\"] == val_df[\"pred_label\"]\n",
    "\n",
    "# ì •í™•ë„ ê³„ì‚°\n",
    "accuracy = val_df[\"correct\"].mean()\n",
    "print(f\"âœ… Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ (ì„ íƒ)\n",
    "val_df.to_csv(\"validation_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ë‹¤ìŒì€ ëª¨ë¸ ë“±ë“± ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"beomi/kcbert-base\", num_labels=5)\n",
    "model = PeftModel.from_pretrained(base_model, \"/content/drive/MyDrive/Models/ToneDetect_adapter\")\n",
    "model.eval()\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/Models/ToneDetect_adapter\")\n",
    "\n",
    "# ê²€ì¦ ì˜ˆì¸¡ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import pandas as pd\n",
    "val_df = pd.read_csv(\"validation_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA ì ìš© í›„ ëª¨ë¸ ì •í™•ë„ ê²€ì¦ í›„ í‹€ë¦° ì˜ˆì¸¡ë§Œ ëª¨ì•„ë³´ëŠ” ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# 1. í‹€ë¦° ì˜ˆì¸¡ë§Œ ì¶”ì¶œ\n",
    "wrong_predictions = val_df[val_df[\"gold_label\"] != val_df[\"pred_label\"]].copy()\n",
    "wrong_predictions = wrong_predictions.reset_index(drop=True)\n",
    "\n",
    "# 3. í‹€ë¦° ì˜ˆì¸¡ ìƒìœ„ 10ê°œ ì¶œë ¥\n",
    "print(\"\\nğŸ” í‹€ë¦° ì˜ˆì¸¡ ì˜ˆì‹œ:\")\n",
    "print(wrong_predictions[[\"sentence\", \"gold_label\", \"pred_label\"]].head(10))\n",
    "\n",
    "# 4. ì–´ë–¤ gold_labelì´ ê°€ì¥ ë§ì´ í‹€ë ¸ëŠ”ê°€?\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=wrong_predictions, x=\"gold_label\", order=wrong_predictions[\"gold_label\"].value_counts().index)\n",
    "plt.title(\"âŒ í‹€ë¦° ì˜ˆì¸¡ - ì‹¤ì œ ë¼ë²¨ ë¶„í¬\")\n",
    "plt.xlabel(\"ì •ë‹µ ë¼ë²¨ (gold)\")\n",
    "plt.ylabel(\"í‹€ë¦° ê°œìˆ˜\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Confusion Matrix ì‹œê°í™”\n",
    "print(\"\\nğŸ“Š Confusion Matrix (ì „ì²´ validation set ê¸°ì¤€):\")\n",
    "\n",
    "cm = confusion_matrix(val_df[\"gold_label\"], val_df[\"pred_label\"], labels=val_df[\"gold_label\"].unique())\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_df[\"gold_label\"].unique())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp.plot(xticks_rotation=45, cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
