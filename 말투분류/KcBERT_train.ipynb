{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            style                     sentence\n",
      "0          formal       안녕하세요. 저는 고양이 6마리 키워요.\n",
      "1          formal     고양이를 6마리나요? 키우는거 안 힘드세요?\n",
      "2          formal  제가 워낙 고양이를 좋아해서 크게 힘들진 않아요.\n",
      "3          formal       가장 나이가 많은 고양이가 어떻게 돼요?\n",
      "4          formal           여섯 살입니다. 갈색 고양이에요.\n",
      "...           ...                          ...\n",
      "62980  translator                          NaN\n",
      "62981  translator                          NaN\n",
      "62982  translator                          NaN\n",
      "62983  translator                          NaN\n",
      "62984  translator                          NaN\n",
      "\n",
      "[62985 rows x 2 columns]\n",
      "            style                                 sentence  label\n",
      "0          formal                   안녕하세요. 저는 고양이 6마리 키워요.      6\n",
      "1          formal                 고양이를 6마리나요? 키우는거 안 힘드세요?      6\n",
      "2          formal              제가 워낙 고양이를 좋아해서 크게 힘들진 않아요.      6\n",
      "3          formal                   가장 나이가 많은 고양이가 어떻게 돼요?      6\n",
      "4          formal                       여섯 살입니다. 갈색 고양이에요.      6\n",
      "...           ...                                      ...    ...\n",
      "36789  translator                        진행되는 공부의 종류는 무엇인?     16\n",
      "36790  translator              나는 수학, 영어, 국어와 같은 수업들을 듣는다.     16\n",
      "36791  translator                      당신이 가장 좋아하는 수업은 무엇?     16\n",
      "36792  translator                개인적으로, 나는 체육 수업의 큰 애호가이다.     16\n",
      "36793  translator  그것은 재미있는 것으로 들리는, 나 또한 학교에서 축구를 자주 즐겼다.     16\n",
      "\n",
      "[36794 rows x 3 columns]\n",
      "라벨 개수: 17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_csv(\"smilestyle_dataset.tsv\", sep=\"\\t\")\n",
    "\n",
    "# 병합 기준 정의\n",
    "merge_map = {\n",
    "    \"formal\": \"formal\",\n",
    "    \"seonbi\": \"formal\",\n",
    "    \"translator\": \"formal\",\n",
    "\n",
    "    \"informal\": \"informal\",\n",
    "    \"azae\": \"informal\",\n",
    "    \"choding\": \"informal\",\n",
    "    \"joongding\": \"informal\",\n",
    "\n",
    "    \"chat\": \"chat_emoticon\",\n",
    "    \"emoticon\": \"chat_emoticon\",\n",
    "    \"enfp\": \"chat_emoticon\",\n",
    "\n",
    "    \"gentle\": \"soft_polite\",\n",
    "    \"sosim\": \"soft_polite\",\n",
    "\n",
    "    \"halbae\": \"elder_speech\",\n",
    "    \"halmae\": \"elder_speech\",\n",
    "}\n",
    "\n",
    "# 삭제 대상 라벨\n",
    "drop_labels = [\"king\", \"naruto\", \"android\"]\n",
    "\n",
    "# 데이터셋 병합 및 삭제 처리 함수\n",
    "def flatten_and_merge_labels(df):\n",
    "    records = []\n",
    "    for col in df.columns:\n",
    "        if col in drop_labels:\n",
    "            continue\n",
    "        merged_label = merge_map.get(col, None)\n",
    "        if merged_label:\n",
    "            for sentence in df[col].dropna():\n",
    "                records.append({\"label\": merged_label, \"sentence\": sentence})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# 적용\n",
    "df = pd.read_csv(\"smilestyle_dataset.tsv\", sep=\"\\t\")\n",
    "df = flatten_and_merge_labels(df)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # 데이터 섞기\n",
    "\n",
    "# 라벨 인코딩\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])  # 스타일을 숫자로 변환\n",
    "df = df.dropna(subset=[\"sentence\"]).reset_index(drop=True)\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "\n",
    "# 확인\n",
    "print(df.head())\n",
    "print(\"라벨 개수:\", num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 29435/29435 [00:03<00:00, 7855.85 examples/s]\n",
      "Map: 100%|██████████| 7359/7359 [00:00<00:00, 7590.17 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'style': 'king', 'sentence': '참으로 사악한 장사인지고!', 'labels': tensor(12), '__index_level_0__': tensor(28515), 'input_ids': tensor([    2, 10133, 13783,  9774,  8148,  4034,     5,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")\n",
    "\n",
    "# 토크나이즈 함수 수정 (sentence 열 사용)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# 8:2로 나누기 (라벨 비율 유지, 재현성 있는 분할)\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Hugging Face Dataset 객체로 변환\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# 토크나이징\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_val = tokenized_val.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_val.set_format(\"torch\")\n",
    "\n",
    "example = tokenized_train.with_format(\"python\")[0]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"beomi/kcbert-base\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 307,985 || all params: 109,239,586 || trainable%: 0.2819\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS  # 문장 분류\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\김소연\\ToneDetect\\venv\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ① Base model 저장 (로라 adapter 적용 전 원래 모델)\n",
    "base_model = model.base_model.model\n",
    "base_model.save_pretrained(\"ToneDetect_base\")\n",
    "\n",
    "# ② Adapter 가중치 + tokenizer 저장\n",
    "model.save_pretrained(\"ToneDetect_adapter\")\n",
    "tokenizer.save_pretrained(\"ToneDetect_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 다음은 정확도 검증용 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# PEFT 적용된 모델 로드 (이전 학습된 것)\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "adapter_path = \"/content/drive/MyDrive/Models/ToneDetect_adapter\"\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"beomi/kcbert-base\", num_labels=5)\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "model.eval()\n",
    "\n",
    "# validation 데이터셋 불러오기\n",
    "df = pd.read_csv(\"smilestyle_dataset.tsv\", sep=\"\\t\")\n",
    "# merge 및 전처리 작업 생략 – 기존 코드로 flatten_and_merge_labels 함수 사용\n",
    "\n",
    "# 전처리\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = flatten_and_merge_labels(df)\n",
    "df = df.dropna(subset=[\"sentence\"]).reset_index(drop=True)\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"label\"])\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "# 역변환을 위한 맵\n",
    "idx2label = dict(zip(range(len(le.classes_)), le.inverse_transform(range(len(le.classes_)))))\n",
    "\n",
    "# 예측 수행\n",
    "predictions = []\n",
    "for i, row in tqdm(val_df.iterrows(), total=len(val_df)):\n",
    "    inputs = tokenizer(row[\"sentence\"], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    predictions.append(pred)\n",
    "\n",
    "# 결과 저장 및 정확도 계산\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "val_df[\"gold_label\"] = val_df[\"label\"].map(idx2label)\n",
    "val_df[\"pred_label\"] = [idx2label[p] for p in predictions]\n",
    "val_df[\"correct\"] = val_df[\"gold_label\"] == val_df[\"pred_label\"]\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = val_df[\"correct\"].mean()\n",
    "print(f\"✅ Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 결과 저장 (선택)\n",
    "val_df.to_csv(\"validation_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 다음은 모델 등등 불러오는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "# 모델 불러오기\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"beomi/kcbert-base\", num_labels=5)\n",
    "model = PeftModel.from_pretrained(base_model, \"/content/drive/MyDrive/Models/ToneDetect_adapter\")\n",
    "model.eval()\n",
    "\n",
    "# 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/Models/ToneDetect_adapter\")\n",
    "\n",
    "# 검증 예측 결과 불러오기\n",
    "import pandas as pd\n",
    "val_df = pd.read_csv(\"validation_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA 적용 후 모델 정확도 검증 후 틀린 예측만 모아보는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# 1. 틀린 예측만 추출\n",
    "wrong_predictions = val_df[val_df[\"gold_label\"] != val_df[\"pred_label\"]].copy()\n",
    "wrong_predictions = wrong_predictions.reset_index(drop=True)\n",
    "\n",
    "# 3. 틀린 예측 상위 10개 출력\n",
    "print(\"\\n🔍 틀린 예측 예시:\")\n",
    "print(wrong_predictions[[\"sentence\", \"gold_label\", \"pred_label\"]].head(10))\n",
    "\n",
    "# 4. 어떤 gold_label이 가장 많이 틀렸는가?\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=wrong_predictions, x=\"gold_label\", order=wrong_predictions[\"gold_label\"].value_counts().index)\n",
    "plt.title(\"❌ 틀린 예측 - 실제 라벨 분포\")\n",
    "plt.xlabel(\"정답 라벨 (gold)\")\n",
    "plt.ylabel(\"틀린 개수\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Confusion Matrix 시각화\n",
    "print(\"\\n📊 Confusion Matrix (전체 validation set 기준):\")\n",
    "\n",
    "cm = confusion_matrix(val_df[\"gold_label\"], val_df[\"pred_label\"], labels=val_df[\"gold_label\"].unique())\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_df[\"gold_label\"].unique())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp.plot(xticks_rotation=45, cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
